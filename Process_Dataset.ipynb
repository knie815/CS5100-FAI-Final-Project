{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dc4298-db07-4414-8f63-b969cdc5f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.12/site-packages (18.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cc8483-e8d5-49a1-aff8-c9e1087f1de5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train-00000-of-00001.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Parquet file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m      8\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/fai/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/fai/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/fai/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Downloads/fai/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-00000-of-00001.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "parquet_file = 'train-00000-of-00001.parquet'\n",
    "data = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = 'train.csv'\n",
    "data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"File converted to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c664689-eb49-4921-b86d-507b5b22417d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='train.csv' target='_blank'>train.csv</a><br>"
      ],
      "text/plain": [
       "/home/katherine/Downloads/train.csv"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Use Jupyter's file download widget\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Display the direct download option\n",
    "FileLink(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ee641c-859a-4899-9d4c-fc343447e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions1 DataFrame Head:\n",
      "                                                text  label\n",
      "0      i just feel really helpless and heavy hearted      4\n",
      "1  ive enjoyed being able to slouch about relax a...      0\n",
      "2  i gave up my internship with the dmrg and am f...      4\n",
      "3                         i dont know i feel so lost      0\n",
      "4  i am a kindergarten teacher and i am thoroughl...      4\n",
      "\n",
      "Emotions2 DataFrame Head:\n",
      "                                                text  emotion\n",
      "0  i should have been at the pub instead of which...  sadness\n",
      "1  ill just have to make some local friends i can...    anger\n",
      "2                                i didnt feel so hot     love\n",
      "3  i would further suggest people might feel more...     love\n",
      "4                i am feeling irritable cranky often    anger\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "emotions1_df = pd.read_csv('emotions.csv')\n",
    "emotions2_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Print the first few rows of each dataframe\n",
    "print(\"Emotions1 DataFrame Head:\")\n",
    "print(emotions1_df.head())\n",
    "\n",
    "print(\"\\nEmotions2 DataFrame Head:\")\n",
    "print(emotions2_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920d31ae-f3f5-432f-93de-c0686ec49d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions DataFrame with Categorical Labels:\n",
      "                                                text  emotion\n",
      "0  i should have been at the pub instead of which...        0\n",
      "1  ill just have to make some local friends i can...        3\n",
      "2                                i didnt feel so hot        2\n",
      "3  i would further suggest people might feel more...        2\n",
      "4                i am feeling irritable cranky often        3\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from numeric labels to emotion words\n",
    "label_mapping = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "label_mapping_2 = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "# emotions1_df['label'] = emotions1_df['label'].map(label_mapping)\n",
    "emotions2_df['emotion'] = emotions2_df['emotion'].map(label_mapping_2)\n",
    "\n",
    "# Print the first few rows to verify the change\n",
    "print(\"Emotions DataFrame with Categorical Labels:\")\n",
    "print(emotions2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2a9550f-d31b-4e02-9049-46524400598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  i should have been at the pub instead of which...      0\n",
      "1  ill just have to make some local friends i can...      3\n",
      "2                                i didnt feel so hot      2\n",
      "3  i would further suggest people might feel more...      2\n",
      "4                i am feeling irritable cranky often      3\n"
     ]
    }
   ],
   "source": [
    "# Rename the column 'emotions' to 'label' in train_df\n",
    "emotions2_df = emotions2_df.rename(columns={'emotion': 'label'})\n",
    "\n",
    "# Verify the change\n",
    "print(emotions2_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf7e418-cff8-4436-8c89-cbbd9091b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in emotions_df: 416809\n",
      "Number of rows in train_df: 3432\n"
     ]
    }
   ],
   "source": [
    "# Count rows before appending\n",
    "emotions1_count = emotions1_df.shape[0]\n",
    "emotions2_count = emotions2_df.shape[0]\n",
    "print(f\"Number of rows in emotions_df: {emotions1_count}\")\n",
    "print(f\"Number of rows in train_df: {emotions2_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86a5c486-8370-4ce0-a439-e8e348c5de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame:\n",
      "                                                text  label\n",
      "0      i just feel really helpless and heavy hearted      4\n",
      "1  ive enjoyed being able to slouch about relax a...      0\n",
      "2  i gave up my internship with the dmrg and am f...      4\n",
      "3                         i dont know i feel so lost      0\n",
      "4  i am a kindergarten teacher and i am thoroughl...      4\n",
      "Number of rows in emotions_df: 420241\n"
     ]
    }
   ],
   "source": [
    "# Ensure they have the same columns if needed\n",
    "combined_df = pd.concat([emotions1_df, emotions2_df], ignore_index=True)\n",
    "\n",
    "# Print the first few rows of the combined dataframe\n",
    "print(\"Combined DataFrame:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "emotions_count = combined_df.shape[0]\n",
    "print(f\"Number of rows in emotions_df: {emotions_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bb36f67-b70d-4111-a68f-fc9d4462ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/katherine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/katherine/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "negation_words = ['no','nor','not']\n",
    "\n",
    "# Initialize lemmatizer, stemmer, and stop words list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define the text preprocessing function with stemming\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, digits, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove lemmatize, and stem remaining words\n",
    "    processed_words = [\n",
    "        stemmer.stem(lemmatizer.lemmatize(word))  # Apply both lemmatization and stemming\n",
    "        for word in words if len(word) > 1 and (word not in stop_words or word in negation_words)# Remove one letter words like \"t\" and stopwords except negation\n",
    "    ]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(processed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "275cf0a4-a6a4-4aac-9399-707e07a0c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Combined DataFrame with processed text:\n",
      "                                                text  label  \\\n",
      "0      i just feel really helpless and heavy hearted      4   \n",
      "1  ive enjoyed being able to slouch about relax a...      0   \n",
      "2  i gave up my internship with the dmrg and am f...      4   \n",
      "3                         i dont know i feel so lost      0   \n",
      "4  i am a kindergarten teacher and i am thoroughl...      4   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0                   feel realli helpless heavi heart  \n",
      "1  ive enjoy abl slouch relax unwind frankli need...  \n",
      "2               gave internship dmrg feel distraught  \n",
      "3                                dont know feel lost  \n",
      "4  kindergarten teacher thoroughli weari job take...  \n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to the 'text' column in the combined DataFrame\n",
    "combined_df['preprocessed_text'] = combined_df['text'].apply(preprocess_text)\n",
    "# Print the first few rows of the updated DataFrame\n",
    "print(\"\\nUpdated Combined DataFrame with processed text:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7f02d1-2887-41ab-aeb9-ec9cfa68e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('processed_emotions_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "360b7d52-0655-4581-afa6-a2de83d67029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
