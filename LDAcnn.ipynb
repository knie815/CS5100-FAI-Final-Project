{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hlPlyGY0nRQ",
        "outputId": "56af31af-2e88-4099-e7ca-8bdee70c8359"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m10506/10506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 70ms/step - accuracy: 0.8216 - loss: 0.4676 - val_accuracy: 0.9171 - val_loss: 0.1581\n",
            "Epoch 2/5\n",
            "\u001b[1m10506/10506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 70ms/step - accuracy: 0.9161 - loss: 0.1752 - val_accuracy: 0.9175 - val_loss: 0.1542\n",
            "Epoch 3/5\n",
            "\u001b[1m10506/10506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 66ms/step - accuracy: 0.9220 - loss: 0.1608 - val_accuracy: 0.9183 - val_loss: 0.1552\n",
            "Epoch 4/5\n",
            "\u001b[1m10506/10506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 65ms/step - accuracy: 0.9248 - loss: 0.1540 - val_accuracy: 0.9178 - val_loss: 0.1622\n",
            "Epoch 5/5\n",
            "\u001b[1m10506/10506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 66ms/step - accuracy: 0.9264 - loss: 0.1486 - val_accuracy: 0.9195 - val_loss: 0.1641\n",
            "\u001b[1m2627/2627\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.9188 - loss: 0.1650\n",
            "Test accuracy: 91.95%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "Predicted Label for the test text: 1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('processed_emotions_dataset_2.csv', index_col=0)\n",
        "data = data.dropna()  # Drop missing values\n",
        "docs = data['text']  # Text column from dataset\n",
        "labels = data['label']  # Labels (emotions)\n",
        "\n",
        "# Define stopwords and additional custom stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "negation_words = ['not', \"don't\", 'no', 'never', \"can't\", \"won't\"]\n",
        "additional_stopwords = {\n",
        "    'really', 'very', 'totally', 'absolutely', 'so', 'quite', 'extremely',\n",
        "    'incredibly', 'super', 'definitely', 'omg', 'lol', 'lmao', 'brb', 'fml',\n",
        "    'idk', 'smh', 'tbh', 'like', 'you', 'know', 'um', 'literally', 'actually',\n",
        "    'basically', 'kind', 'sorta', 'might', 'would', 'could', 'can', 'should',\n",
        "    'not', 'no', 'don’t', 'never', 'nothing', 'feel', 'felt', 'wish', 'hope',\n",
        "    'maybe', 'perhaps', 'im', 'uh', 'oh', 'wow', 'ouch', 'eh', 'whoa', 'yikes'\n",
        "}\n",
        "stop_words.update(additional_stopwords)\n",
        "\n",
        "# Preprocessing function for tokenization and filtering\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = []\n",
        "    negation = False\n",
        "    for word in tokens:\n",
        "        if word in negation_words:\n",
        "            negation = not negation  # Flip negation status\n",
        "            continue  # Skip the negation word itself\n",
        "        if word.isalpha() and word not in stop_words:\n",
        "            if negation:\n",
        "                word = 'not_' + word  # Mark the word as negated\n",
        "            filtered_tokens.append(word)\n",
        "            negation = False  # Reset negation after the word\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_docs = [preprocess_text(doc) for doc in docs]\n",
        "\n",
        "# Tokenization and padding setup\n",
        "max_words = 10000  # Maximum number of words in the vocabulary\n",
        "max_sequence_length = 100  # Maximum length of each input sequence\n",
        "\n",
        "# Tokenizer to convert text into sequences of integers\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(preprocessed_docs)\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_docs)\n",
        "\n",
        "# Pad sequences to ensure consistent input size\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y = to_categorical(labels, num_classes=6)  # 6 emotion classes\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='softmax'))  # 6 emotions classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Optional: Make predictions on a new example\n",
        "test_text = \"I am feeling great today!\"\n",
        "processed_test_text = preprocess_text(test_text)\n",
        "test_sequence = tokenizer.texts_to_sequences([processed_test_text])\n",
        "test_padded = pad_sequences(test_sequence, maxlen=max_sequence_length)\n",
        "prediction = model.predict(test_padded)\n",
        "predicted_label = np.argmax(prediction, axis=1)\n",
        "print(f\"Predicted Label for the test text: {predicted_label[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG16j87h4gCX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}