{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0      i just feel really helpless and heavy hearted      fear   \n",
      "1  ive enjoyed being able to slouch about relax a...   sadness   \n",
      "2  i gave up my internship with the dmrg and am f...      fear   \n",
      "3                         i dont know i feel so lost   sadness   \n",
      "4  i am a kindergarten teacher and i am thoroughl...      fear   \n",
      "5         i was beginning to feel quite disheartened   sadness   \n",
      "6  i would think that whomever would be lucky eno...      love   \n",
      "7  i fear that they won t ever feel that deliciou...       joy   \n",
      "8  im forever taking some time out to have a lie ...  surprise   \n",
      "9  i can still lose the weight without feeling de...   sadness   \n",
      "\n",
      "                                   preprocessed_text  \\\n",
      "0          just feel realli helpless and heavi heart   \n",
      "1  ive enjoy be abl to slouch about relax and unw...   \n",
      "2  gave up my internship with the dmrg and am fee...   \n",
      "3                             dont know feel so lost   \n",
      "4  am kindergarten teacher and am thoroughli wear...   \n",
      "5                   wa begin to feel quit dishearten   \n",
      "6  would think that whomev would be lucki enough ...   \n",
      "7  fear that they won ever feel that delici excit...   \n",
      "8  im forev take some time out to have lie down b...   \n",
      "9      can still lose the weight without feel depriv   \n",
      "\n",
      "                             preprocessed_text_split  \n",
      "0  [just, feel, realli, helpless, and, heavi, heart]  \n",
      "1  [ive, enjoy, be, abl, to, slouch, about, relax...  \n",
      "2  [gave, up, my, internship, with, the, dmrg, an...  \n",
      "3                       [dont, know, feel, so, lost]  \n",
      "4  [am, kindergarten, teacher, and, am, thoroughl...  \n",
      "5            [wa, begin, to, feel, quit, dishearten]  \n",
      "6  [would, think, that, whomev, would, be, lucki,...  \n",
      "7  [fear, that, they, won, ever, feel, that, deli...  \n",
      "8  [im, forev, take, some, time, out, to, have, l...  \n",
      "9  [can, still, lose, the, weight, without, feel,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "preprocessed_data = pd.read_csv('processed_emotions_dataset.csv',index_col=0)\n",
    "preprocessed_data['preprocessed_text_split'] = preprocessed_data['preprocessed_text'].str.split()\n",
    "preprocessed_data = preprocessed_data.dropna()\n",
    "print(preprocessed_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After segregating and taking equal number of rows for each sentiment:\n",
      "label\n",
      "joy         15000\n",
      "sadness     15000\n",
      "anger       15000\n",
      "fear        15000\n",
      "love        15000\n",
      "surprise    15000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i fear that they won t ever feel that deliciou...</td>\n",
       "      <td>joy</td>\n",
       "      <td>fear that they won ever feel that delici excit...</td>\n",
       "      <td>[fear, that, they, won, ever, feel, that, deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i try to be nice though so if you get a bitchy...</td>\n",
       "      <td>joy</td>\n",
       "      <td>tri to be nice though so if you get bitchi per...</td>\n",
       "      <td>[tri, to, be, nice, though, so, if, you, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i have officially graduated im not feeling as ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>have offici graduat im not feel a ecstat a tho...</td>\n",
       "      <td>[have, offici, graduat, im, not, feel, a, ecst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i feel my portfolio demonstrates how eager i a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>feel my portfolio demonstr how eager am to lea...</td>\n",
       "      <td>[feel, my, portfolio, demonstr, how, eager, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i may be more biased than the next because i h...</td>\n",
       "      <td>joy</td>\n",
       "      <td>may be more bias than the next becaus have dep...</td>\n",
       "      <td>[may, be, more, bias, than, the, next, becaus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i didn t feel terrific</td>\n",
       "      <td>joy</td>\n",
       "      <td>didn feel terrif</td>\n",
       "      <td>[didn, feel, terrif]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i am feeling much stronger and more confident ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>am feel much stronger and more confid now and ...</td>\n",
       "      <td>[am, feel, much, stronger, and, more, confid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i take a shower i feel wonderful energetic and...</td>\n",
       "      <td>joy</td>\n",
       "      <td>take shower feel wonder energet and all my pre...</td>\n",
       "      <td>[take, shower, feel, wonder, energet, and, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>i feel like i am actually getting something us...</td>\n",
       "      <td>joy</td>\n",
       "      <td>feel like am actual get someth use out of it</td>\n",
       "      <td>[feel, like, am, actual, get, someth, use, out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i was able to overcome this anxiousness and fe...</td>\n",
       "      <td>joy</td>\n",
       "      <td>wa abl to overcom thi anxious and feel peac a ...</td>\n",
       "      <td>[wa, abl, to, overcom, thi, anxious, and, feel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label  \\\n",
       "7   i fear that they won t ever feel that deliciou...   joy   \n",
       "10  i try to be nice though so if you get a bitchy...   joy   \n",
       "12  i have officially graduated im not feeling as ...   joy   \n",
       "14  i feel my portfolio demonstrates how eager i a...   joy   \n",
       "15  i may be more biased than the next because i h...   joy   \n",
       "16                             i didn t feel terrific   joy   \n",
       "21  i am feeling much stronger and more confident ...   joy   \n",
       "22  i take a shower i feel wonderful energetic and...   joy   \n",
       "26  i feel like i am actually getting something us...   joy   \n",
       "27  i was able to overcome this anxiousness and fe...   joy   \n",
       "\n",
       "                                    preprocessed_text  \\\n",
       "7   fear that they won ever feel that delici excit...   \n",
       "10  tri to be nice though so if you get bitchi per...   \n",
       "12  have offici graduat im not feel a ecstat a tho...   \n",
       "14  feel my portfolio demonstr how eager am to lea...   \n",
       "15  may be more bias than the next becaus have dep...   \n",
       "16                                   didn feel terrif   \n",
       "21  am feel much stronger and more confid now and ...   \n",
       "22  take shower feel wonder energet and all my pre...   \n",
       "26       feel like am actual get someth use out of it   \n",
       "27  wa abl to overcom thi anxious and feel peac a ...   \n",
       "\n",
       "                              preprocessed_text_split  \n",
       "7   [fear, that, they, won, ever, feel, that, deli...  \n",
       "10  [tri, to, be, nice, though, so, if, you, get, ...  \n",
       "12  [have, offici, graduat, im, not, feel, a, ecst...  \n",
       "14  [feel, my, portfolio, demonstr, how, eager, am...  \n",
       "15  [may, be, more, bias, than, the, next, becaus,...  \n",
       "16                               [didn, feel, terrif]  \n",
       "21  [am, feel, much, stronger, and, more, confid, ...  \n",
       "22  [take, shower, feel, wonder, energet, and, all...  \n",
       "26  [feel, like, am, actual, get, someth, use, out...  \n",
       "27  [wa, abl, to, overcom, thi, anxious, and, feel...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to retrieve top few number of each category\n",
    "def get_top_data(preprocessed_data, top_n):\n",
    "    top_data_joy = preprocessed_data[preprocessed_data['label'] == 'joy'].head(top_n)\n",
    "    top_data_sadness = preprocessed_data[preprocessed_data['label'] == 'sadness'].head(top_n)\n",
    "    top_data_anger = preprocessed_data[preprocessed_data['label'] == 'anger'].head(top_n)\n",
    "    top_data_fear = preprocessed_data[preprocessed_data['label'] == 'fear'].head(top_n)\n",
    "    top_data_love = preprocessed_data[preprocessed_data['label'] == 'love'].head(top_n)\n",
    "    top_data_surprise = preprocessed_data[preprocessed_data['label'] == 'surprise'].head(top_n)\n",
    "    data_equal_size_per_label = pd.concat([top_data_joy, top_data_sadness, top_data_anger, top_data_fear, top_data_love, top_data_surprise])\n",
    "    return data_equal_size_per_label\n",
    "\n",
    "# Function call to get the top 15000 from each sentiment\n",
    "data_equal_size_per_label = get_top_data(preprocessed_data, top_n=15000)\n",
    "\n",
    "# After selecting top few samples of each sentiment\n",
    "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
    "print(data_equal_size_per_label['label'].value_counts())\n",
    "data_equal_size_per_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 19.15617799758911\n"
     ]
    }
   ],
   "source": [
    "# Skip-gram model (sg = 1)\n",
    "size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "word2vec_model_file = 'word2vec_' + str(size) + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(data_equal_size_per_label['preprocessed_text_split']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)\n",
    "\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24348\n",
      "Length of the vector generated for the word nice\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Total number of the words \n",
    "print(len(sg_w2v_model.wv))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for the word nice\")\n",
    "print(len(sg_w2v_model.wv['nice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments\n",
      "label\n",
      "fear        10572\n",
      "love        10546\n",
      "sadness     10545\n",
      "joy         10508\n",
      "anger       10427\n",
      "surprise    10402\n",
      "Name: count, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "label\n",
      "surprise    4598\n",
      "anger       4573\n",
      "joy         4492\n",
      "sadness     4455\n",
      "love        4454\n",
      "fear        4428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split Function\n",
    "def split_train_test(data_equal_size_per_label, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data_equal_size_per_label[['text','preprocessed_text_split']], \n",
    "                                                        data_equal_size_per_label['label'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    # X_train = X_train.reset_index()\n",
    "    # X_test = X_test.reset_index()\n",
    "    # Y_train = Y_train.to_frame()\n",
    "    # Y_train = Y_train.reset_index()\n",
    "    # Y_test = Y_test.to_frame()\n",
    "    # Y_test = Y_test.reset_index()\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(data_equal_size_per_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in following file\n",
    "word2vec_filename = 'train_word2vec.csv'\n",
    "with open(word2vec_filename, 'w') as word2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = (np.mean([sg_w2v_model.wv[token] for token in row['preprocessed_text_split']], axis=0)).tolist() # this is where we can add the probabliities from LDA to do a weighted avg instead\n",
    "        # if index == 0:\n",
    "        #     header = \",\".join(str(ele) for ele in range(1000))\n",
    "        #     word2vec_file.write(header)\n",
    "        #     word2vec_file.write(\"\\n\")\n",
    "        # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model with word2vec vectors: 99.00531816482544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.24      0.24      0.24      4573\n",
      "        fear       0.22      0.24      0.23      4428\n",
      "         joy       0.25      0.25      0.25      4492\n",
      "        love       0.29      0.29      0.29      4454\n",
      "     sadness       0.25      0.24      0.25      4455\n",
      "    surprise       0.32      0.30      0.31      4598\n",
      "\n",
      "    accuracy                           0.26     27000\n",
      "   macro avg       0.26      0.26      0.26     27000\n",
      "weighted avg       0.26      0.26      0.26     27000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load from the filename\n",
    "word2vec_df = pd.read_csv(word2vec_filename, header=None)\n",
    "#Initialize the model\n",
    "clf_decision_word2vec = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "# Fit the model\n",
    "clf_decision_word2vec.fit(word2vec_df, Y_train)\n",
    "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))\n",
    "\n",
    "# Test model\n",
    "test_features_word2vec = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector = np.mean([sg_w2v_model.wv[token] for token in row['preprocessed_text_split']], axis=0).tolist() # this is where we can add the probabliities from LDA to do a weighted avg instead\n",
    "    if type(model_vector) is list:\n",
    "        test_features_word2vec.append(model_vector)\n",
    "    else:\n",
    "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
    "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
    "print(classification_report(Y_test,test_predictions_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136748</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.126728</td>\n",
       "      <td>0.141341</td>\n",
       "      <td>-0.019738</td>\n",
       "      <td>-0.013882</td>\n",
       "      <td>0.028697</td>\n",
       "      <td>0.067027</td>\n",
       "      <td>-0.059400</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>-0.044032</td>\n",
       "      <td>0.082148</td>\n",
       "      <td>-0.005767</td>\n",
       "      <td>0.077980</td>\n",
       "      <td>0.070310</td>\n",
       "      <td>-0.100464</td>\n",
       "      <td>-0.096644</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>-0.063679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081536</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.142125</td>\n",
       "      <td>0.125711</td>\n",
       "      <td>-0.049791</td>\n",
       "      <td>-0.060880</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>-0.122056</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013020</td>\n",
       "      <td>-0.043412</td>\n",
       "      <td>0.065545</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.039328</td>\n",
       "      <td>0.039257</td>\n",
       "      <td>-0.087567</td>\n",
       "      <td>-0.021173</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>-0.065973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113411</td>\n",
       "      <td>0.054585</td>\n",
       "      <td>0.150720</td>\n",
       "      <td>0.182355</td>\n",
       "      <td>-0.029868</td>\n",
       "      <td>-0.011712</td>\n",
       "      <td>0.074743</td>\n",
       "      <td>0.032017</td>\n",
       "      <td>-0.075775</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051781</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>-0.009556</td>\n",
       "      <td>0.115662</td>\n",
       "      <td>0.035854</td>\n",
       "      <td>-0.156091</td>\n",
       "      <td>-0.051777</td>\n",
       "      <td>0.100583</td>\n",
       "      <td>-0.015398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.093993</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.115904</td>\n",
       "      <td>0.112445</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.077514</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>-0.056403</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014005</td>\n",
       "      <td>-0.086076</td>\n",
       "      <td>0.089747</td>\n",
       "      <td>-0.007296</td>\n",
       "      <td>0.056042</td>\n",
       "      <td>0.068427</td>\n",
       "      <td>-0.073300</td>\n",
       "      <td>-0.068122</td>\n",
       "      <td>0.023981</td>\n",
       "      <td>-0.089070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.083298</td>\n",
       "      <td>0.040043</td>\n",
       "      <td>0.184852</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>-0.024539</td>\n",
       "      <td>-0.029961</td>\n",
       "      <td>0.147358</td>\n",
       "      <td>0.087780</td>\n",
       "      <td>-0.125293</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060195</td>\n",
       "      <td>-0.016713</td>\n",
       "      <td>0.141719</td>\n",
       "      <td>-0.043830</td>\n",
       "      <td>0.109405</td>\n",
       "      <td>0.057650</td>\n",
       "      <td>-0.139219</td>\n",
       "      <td>-0.082996</td>\n",
       "      <td>0.048123</td>\n",
       "      <td>-0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62995</th>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.036803</td>\n",
       "      <td>0.169598</td>\n",
       "      <td>0.149909</td>\n",
       "      <td>-0.029449</td>\n",
       "      <td>-0.027888</td>\n",
       "      <td>0.056955</td>\n",
       "      <td>0.054978</td>\n",
       "      <td>-0.054855</td>\n",
       "      <td>0.073228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107963</td>\n",
       "      <td>-0.065031</td>\n",
       "      <td>0.215270</td>\n",
       "      <td>0.027194</td>\n",
       "      <td>0.206374</td>\n",
       "      <td>0.055063</td>\n",
       "      <td>-0.175919</td>\n",
       "      <td>-0.095489</td>\n",
       "      <td>0.055149</td>\n",
       "      <td>-0.064701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62996</th>\n",
       "      <td>0.113365</td>\n",
       "      <td>0.028381</td>\n",
       "      <td>0.127115</td>\n",
       "      <td>0.140026</td>\n",
       "      <td>-0.030701</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.083048</td>\n",
       "      <td>-0.045503</td>\n",
       "      <td>0.023704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005925</td>\n",
       "      <td>-0.019372</td>\n",
       "      <td>0.068567</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>0.062238</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>-0.068164</td>\n",
       "      <td>0.029360</td>\n",
       "      <td>-0.052166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62997</th>\n",
       "      <td>0.067098</td>\n",
       "      <td>0.030910</td>\n",
       "      <td>0.112865</td>\n",
       "      <td>0.148546</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>-0.015066</td>\n",
       "      <td>0.103125</td>\n",
       "      <td>0.052034</td>\n",
       "      <td>-0.059157</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049408</td>\n",
       "      <td>-0.078799</td>\n",
       "      <td>0.073377</td>\n",
       "      <td>-0.021775</td>\n",
       "      <td>0.108815</td>\n",
       "      <td>0.058079</td>\n",
       "      <td>-0.114651</td>\n",
       "      <td>-0.060154</td>\n",
       "      <td>0.032671</td>\n",
       "      <td>-0.051571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62998</th>\n",
       "      <td>0.119125</td>\n",
       "      <td>0.025730</td>\n",
       "      <td>0.104771</td>\n",
       "      <td>0.132973</td>\n",
       "      <td>-0.043787</td>\n",
       "      <td>-0.023368</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>-0.046767</td>\n",
       "      <td>0.065681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>-0.033685</td>\n",
       "      <td>0.057660</td>\n",
       "      <td>-0.039281</td>\n",
       "      <td>0.064536</td>\n",
       "      <td>0.070580</td>\n",
       "      <td>-0.043613</td>\n",
       "      <td>-0.083904</td>\n",
       "      <td>0.016732</td>\n",
       "      <td>-0.087801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62999</th>\n",
       "      <td>0.083789</td>\n",
       "      <td>-0.007147</td>\n",
       "      <td>0.116744</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>-0.028937</td>\n",
       "      <td>0.037705</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>-0.035436</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>-0.018493</td>\n",
       "      <td>0.114430</td>\n",
       "      <td>-0.029663</td>\n",
       "      <td>0.092133</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>-0.026060</td>\n",
       "      <td>-0.098682</td>\n",
       "      <td>0.015073</td>\n",
       "      <td>-0.059606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63000 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.136748  0.007968  0.126728  0.141341 -0.019738 -0.013882  0.028697   \n",
       "1      0.081536  0.015476  0.142125  0.125711 -0.049791 -0.060880  0.072929   \n",
       "2      0.113411  0.054585  0.150720  0.182355 -0.029868 -0.011712  0.074743   \n",
       "3      0.093993  0.012024  0.115904  0.112445  0.017658  0.007313  0.077514   \n",
       "4      0.083298  0.040043  0.184852  0.157534 -0.024539 -0.029961  0.147358   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "62995  0.016483  0.036803  0.169598  0.149909 -0.029449 -0.027888  0.056955   \n",
       "62996  0.113365  0.028381  0.127115  0.140026 -0.030701  0.003696  0.013907   \n",
       "62997  0.067098  0.030910  0.112865  0.148546 -0.003980 -0.015066  0.103125   \n",
       "62998  0.119125  0.025730  0.104771  0.132973 -0.043787 -0.023368 -0.001445   \n",
       "62999  0.083789 -0.007147  0.116744  0.138371 -0.016025 -0.028937  0.037705   \n",
       "\n",
       "            7         8         9    ...       990       991       992  \\\n",
       "0      0.067027 -0.059400  0.015920  ...  0.011304 -0.044032  0.082148   \n",
       "1      0.110954 -0.122056  0.021158  ... -0.013020 -0.043412  0.065545   \n",
       "2      0.032017 -0.075775  0.017285  ... -0.051781  0.009105  0.126549   \n",
       "3      0.042433 -0.056403  0.038597  ...  0.014005 -0.086076  0.089747   \n",
       "4      0.087780 -0.125293  0.024725  ... -0.060195 -0.016713  0.141719   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "62995  0.054978 -0.054855  0.073228  ... -0.107963 -0.065031  0.215270   \n",
       "62996  0.083048 -0.045503  0.023704  ... -0.005925 -0.019372  0.068567   \n",
       "62997  0.052034 -0.059157  0.030212  ... -0.049408 -0.078799  0.073377   \n",
       "62998  0.064221 -0.046767  0.065681  ... -0.008335 -0.033685  0.057660   \n",
       "62999  0.019406 -0.035436  0.024008  ...  0.014085 -0.018493  0.114430   \n",
       "\n",
       "            993       994       995       996       997       998       999  \n",
       "0     -0.005767  0.077980  0.070310 -0.100464 -0.096644  0.011750 -0.063679  \n",
       "1      0.006164  0.039328  0.039257 -0.087567 -0.021173  0.038129 -0.065973  \n",
       "2     -0.009556  0.115662  0.035854 -0.156091 -0.051777  0.100583 -0.015398  \n",
       "3     -0.007296  0.056042  0.068427 -0.073300 -0.068122  0.023981 -0.089070  \n",
       "4     -0.043830  0.109405  0.057650 -0.139219 -0.082996  0.048123 -0.001969  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "62995  0.027194  0.206374  0.055063 -0.175919 -0.095489  0.055149 -0.064701  \n",
       "62996 -0.004937  0.074965  0.062238 -0.070393 -0.068164  0.029360 -0.052166  \n",
       "62997 -0.021775  0.108815  0.058079 -0.114651 -0.060154  0.032671 -0.051571  \n",
       "62998 -0.039281  0.064536  0.070580 -0.043613 -0.083904  0.016732 -0.087801  \n",
       "62999 -0.029663  0.092133  0.029923 -0.026060 -0.098682  0.015073 -0.059606  \n",
       "\n",
       "[63000 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
