{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7IpVqTUU29u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "label_mapping = {\n",
        "    0: 'sadness',\n",
        "    1: 'joy',\n",
        "    2: 'love',\n",
        "    3: 'anger',\n",
        "    4: 'fear',\n",
        "    5: 'surprise'\n",
        "}\n",
        "\n",
        "\n",
        "music_recommendations = {\n",
        "    'sadness': ['Melancholic Piano', 'Sad Violin Music'],\n",
        "    'joy': ['Happy Acoustic Guitar', 'Uplifting Piano'],\n",
        "    'love': ['Romantic Piano', 'Love Songs Instrumental'],\n",
        "    'anger': ['Intense Rock Instrumental', 'Heavy Metal Instrumental'],\n",
        "    'fear': ['Dark Cinematic Music', 'Tense Ambient Soundscapes'],\n",
        "    'surprise': ['Energetic Orchestral Music', 'Exciting Electronic Beats'],\n",
        "}\n",
        "\n",
        "\n",
        "data = pd.read_csv('emotions.csv')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    contractions = {\n",
        "        \"dont\": \"do not\",\n",
        "        \"cant\": \"cannot\",\n",
        "        \"wont\": \"will not\",\n",
        "        \"im\": \"i am\",\n",
        "        \"ive\": \"i have\",\n",
        "        \"id\": \"i would\",\n",
        "        \"youre\": \"you are\",\n",
        "        \"isnt\": \"is not\",\n",
        "        \"wasnt\": \"was not\",\n",
        "        \"shouldnt\": \"should not\",\n",
        "        \"couldnt\": \"could not\",\n",
        "        \"doesnt\": \"does not\",\n",
        "        \"havent\": \"have not\",\n",
        "        \"hasnt\": \"has not\",\n",
        "        \"hadnt\": \"had not\",\n",
        "        \"arent\": \"are not\",\n",
        "        \"werent\": \"were not\",\n",
        "        \"wouldnt\": \"would not\",\n",
        "        \"mustnt\": \"must not\",\n",
        "        \"mightnt\": \"might not\",\n",
        "        \"didnt\": \"did not\",\n",
        "        \"neednt\": \"need not\",\n",
        "        \"oughtnt\": \"ought not\",\n",
        "        \"im\": \"i am\",\n",
        "        \"hes\": \"he is\",\n",
        "        \"shes\": \"she is\",\n",
        "        \"its\": \"it is\",\n",
        "        \"thats\": \"that is\",\n",
        "        \"theres\": \"there is\",\n",
        "        \"whats\": \"what is\",\n",
        "        \"wheres\": \"where is\",\n",
        "        \"whos\": \"who is\",\n",
        "        \"theyre\": \"they are\",\n",
        "        \"weve\": \"we have\",\n",
        "        \"were\": \"we are\",\n",
        "        \"didnt\": \"did not\",\n",
        "        \"doesnt\": \"does not\",\n",
        "        \"dont\": \"do not\",\n",
        "        \"hadnt\": \"had not\",\n",
        "        \"hasnt\": \"has not\",\n",
        "        \"havent\": \"have not\",\n",
        "        \"isnt\": \"is not\",\n",
        "        \"shouldnt\": \"should not\",\n",
        "        \"wasnt\": \"was not\",\n",
        "        \"werent\": \"were not\",\n",
        "        \"wont\": \"will not\",\n",
        "        \"wouldnt\": \"would not\",\n",
        "    }\n",
        "\n",
        "\n",
        "    for contraction, replacement in contractions.items():\n",
        "        text = re.sub(r'\\b' + contraction + r'\\b', replacement, text)\n",
        "\n",
        "\n",
        "    tokens = text.split()\n",
        "    tokens = handle_negations(tokens)\n",
        "\n",
        "\n",
        "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token and token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def handle_negations(tokens):\n",
        "    negation_words = set(['no', 'not', 'never', 'none', 'cannot', 'do not', 'dont', 'did not', 'does not', 'didnt', 'doesnt', 'cannot'])\n",
        "    transformed_tokens = []\n",
        "    negate = False\n",
        "    for token in tokens:\n",
        "        if token in negation_words:\n",
        "            negate = True\n",
        "            transformed_tokens.append(token)\n",
        "        elif negate:\n",
        "            transformed_tokens.append('not_' + token)\n",
        "            negate = False\n",
        "        else:\n",
        "            transformed_tokens.append(token)\n",
        "    return transformed_tokens\n",
        "\n",
        "\n",
        "data['processed_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "X = data['processed_text']\n",
        "y = data['label']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "all_words = [word for tokens in X_train for word in tokens]\n",
        "vocab = set(all_words)\n",
        "vocab_size = len(vocab)\n",
        "vocab_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id_to_vocab = {idx: word for word, idx in vocab_to_id.items()}\n",
        "\n",
        "\n",
        "class_priors = {}\n",
        "word_likelihoods = {}\n",
        "alpha = 1\n",
        "\n",
        "for label in label_mapping.keys():\n",
        "\n",
        "    label_docs = X_train[y_train == label]\n",
        "    total_docs = len(X_train)\n",
        "    total_label_docs = len(label_docs)\n",
        "    class_priors[label] = np.log(total_label_docs / total_docs)\n",
        "\n",
        "\n",
        "    words_in_class = [word for tokens in label_docs for word in tokens]\n",
        "    total_words_in_class = len(words_in_class)\n",
        "    word_counts = Counter(words_in_class)\n",
        "\n",
        "\n",
        "    likelihoods = {}\n",
        "    for word in vocab:\n",
        "        count = word_counts.get(word, 0)\n",
        "        likelihoods[word] = np.log((count + alpha) / (total_words_in_class + alpha * vocab_size))\n",
        "    word_likelihoods[label] = likelihoods\n",
        "\n",
        "def handle_negations(tokens):\n",
        "    negation_words = {'no', 'not', 'never', 'none', 'cannot', 'do not', 'dont', 'did not', 'does not', 'didnt', 'doesnt'}\n",
        "    transformed_tokens = []\n",
        "    negate = False\n",
        "    for token in tokens:\n",
        "        if token in negation_words:\n",
        "            negate = True\n",
        "        elif negate:\n",
        "            transformed_tokens.append('not_' + token)\n",
        "            negate = False\n",
        "        else:\n",
        "            transformed_tokens.append(token)\n",
        "    return transformed_tokens\n",
        "\n",
        "def predict_emotion(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    scores = {label: class_priors[label] for label in label_mapping.keys()}\n",
        "    for label in label_mapping.keys():\n",
        "        for token in tokens:\n",
        "            if token.startswith('not_'):\n",
        "\n",
        "                word = token[4:]\n",
        "                if word in vocab:\n",
        "                    scores[label] -= word_likelihoods[label].get(word, 0)\n",
        "\n",
        "                scores[label] += word_likelihoods[label].get(token, np.log(alpha / (alpha * vocab_size)))\n",
        "            else:\n",
        "                scores[label] += word_likelihoods[label].get(token, np.log(alpha / (alpha * vocab_size)))\n",
        "    predicted_label = max(scores, key=scores.get)\n",
        "    return label_mapping[predicted_label]\n",
        "\n",
        "\n",
        "\n",
        "def compute_accuracy():\n",
        "    correct = 0\n",
        "    total = len(X_test)\n",
        "    for text, true_label in zip(X_test, y_test):\n",
        "        predicted_emotion = predict_emotion(' '.join(text))\n",
        "        if predicted_emotion == label_mapping[true_label]:\n",
        "            correct += 1\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "compute_accuracy()\n",
        "\n"
      ]
    }
  ]
}