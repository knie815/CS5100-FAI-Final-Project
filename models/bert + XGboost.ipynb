{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVU514YwSeiK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from xgboost import XGBClassifier\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data = pd.read_csv(\"emotions.csv\")\n",
    "\n",
    "\n",
    "numeric_to_string_mapping = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "string_to_numeric_mapping = {v: k for k, v in numeric_to_string_mapping.items()}\n",
    "\n",
    "data[\"label_string\"] = data[\"label\"].map(numeric_to_string_mapping)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "embeddings = np.array(data[\"processed_text\"].to_list())\n",
    "labels = data[\"label\"].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "clf = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42, tree_method=\"gpu_hist\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": [\" \".join(map(str, x)) for x in X_train],\n",
    "    \"label\": y_train\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"text\": [\" \".join(map(str, x)) for x in X_test],\n",
    "    \"label\": y_test\n",
    "})\n",
    "\n",
    "tokenized_train = train_dataset.map(lambda e: tokenizer(e[\"text\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\n",
    "tokenized_test = test_dataset.map(lambda e: tokenizer(e[\"text\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\n",
    "\n",
    "\n",
    "bert_finetune = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(numeric_to_string_mapping)).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_finetune,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "final_preds = np.argmax(predictions.predictions, axis=1)\n",
    "final_accuracy = accuracy_score(y_test, final_preds)\n",
    "print(f\"Fine-Tuned BERT Accuracy: {final_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
