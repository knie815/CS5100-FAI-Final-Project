{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnfD72byNlhi",
        "outputId": "2c93432f-c665-4f5d-c80a-3f33606ee3a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                     text  label  \\\n",
            "0           i just feel really helpless and heavy hearted      4   \n",
            "1       ive enjoyed being able to slouch about relax a...      0   \n",
            "2       i gave up my internship with the dmrg and am f...      4   \n",
            "3                              i dont know i feel so lost      0   \n",
            "4       i am a kindergarten teacher and i am thoroughl...      4   \n",
            "...                                                   ...    ...   \n",
            "420236  i feel blessed to be able to see that we didn ...      1   \n",
            "420237  i think another reason i love concerts is it i...      1   \n",
            "420238  i usually take on to more protein when i start...      0   \n",
            "420239  i feel that rich people will never understand ...      1   \n",
            "420240  i feel slightly naughty holding this cd seeing...      2   \n",
            "\n",
            "                                        preprocessed_text  \\\n",
            "0                        feel realli helpless heavi heart   \n",
            "1       ive enjoy abl slouch relax unwind frankli need...   \n",
            "2                    gave internship dmrg feel distraught   \n",
            "3                                     dont know feel lost   \n",
            "4       kindergarten teacher thoroughli weari job take...   \n",
            "...                                                   ...   \n",
            "420236                           feel bless abl see anyth   \n",
            "420237  think anoth reason love concert set feel compl...   \n",
            "420238              usual take protein start feel letharg   \n",
            "420239     feel rich peopl never understand cruelti money   \n",
            "420240  feel slightli naughti hold cd see doesnt offic...   \n",
            "\n",
            "                                  preprocessed_text_split  \n",
            "0                  [feel, realli, helpless, heavi, heart]  \n",
            "1       [ive, enjoy, abl, slouch, relax, unwind, frank...  \n",
            "2              [gave, internship, dmrg, feel, distraught]  \n",
            "3                                [dont, know, feel, lost]  \n",
            "4       [kindergarten, teacher, thoroughli, weari, job...  \n",
            "...                                                   ...  \n",
            "420236                     [feel, bless, abl, see, anyth]  \n",
            "420237  [think, anoth, reason, love, concert, set, fee...  \n",
            "420238       [usual, take, protein, start, feel, letharg]  \n",
            "420239  [feel, rich, peopl, never, understand, cruelti...  \n",
            "420240  [feel, slightli, naughti, hold, cd, see, doesn...  \n",
            "\n",
            "[420225 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from pprint import pprint\n",
        "\n",
        "preprocessed_data = pd.read_csv('processed_emotions_dataset_2.csv',index_col=0)\n",
        "preprocessed_data['preprocessed_text_split'] = preprocessed_data['preprocessed_text'].str.split()\n",
        "preprocessed_data = preprocessed_data.dropna()\n",
        "print(preprocessed_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAPm1rwINlh5",
        "outputId": "898a77fb-86eb-4ba1-9d59-4db5dc090d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "1    141636\n",
            "0    121755\n",
            "3     57883\n",
            "4     48281\n",
            "2     35126\n",
            "5     15544\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "label_counts = preprocessed_data['label'].value_counts()\n",
        "print(label_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mFFLtqbNlh8"
      },
      "outputs": [],
      "source": [
        "\n",
        "w2v_model = Word2Vec(sentences=preprocessed_data['preprocessed_text_split'], vector_size=100, window=5, min_count=1, sg=1, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ezcnUTANlh-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def document_vector(words):\n",
        "    words = [word for word in words if word in w2v_model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(100)\n",
        "    '''\n",
        "    integrate LDA here?\n",
        "    '''\n",
        "    return np.mean(w2v_model.wv[words], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLFStf5ANlh_"
      },
      "outputs": [],
      "source": [
        "\n",
        "preprocessed_data['doc_vector'] = preprocessed_data['preprocessed_text_split'].apply(document_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymP9FMIYNliB"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = np.vstack(preprocessed_data['doc_vector'].values)\n",
        "y = preprocessed_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvRFDyvSNliD"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqSI31zMNliI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "dataset_train = CustomDataset(torch.from_numpy(X_train), torch.tensor(y_train.to_list()))\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4RlJ4QmNliK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FF_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(100, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 6),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.linear_relu_stack(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "feedforward_net = FF_Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ffn = torch.optim.Adam(feedforward_net.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96lCsiZuNliM",
        "outputId": "54bc1c67-e02d-434e-e111-4c44758ae048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 18390.042159244418\n",
            "Training loss: 15822.920703321695\n",
            "Training loss: 14997.72575198114\n",
            "Training loss: 14552.11619593203\n",
            "Training loss: 14247.098839044571\n",
            "Training loss: 14010.443231776357\n",
            "Training loss: 13838.617416538298\n",
            "Training loss: 13693.30406486243\n",
            "Training loss: 13570.959481112659\n",
            "Training loss: 13485.723673276603\n",
            "Training loss: 13403.920531377196\n",
            "Training loss: 13328.965324550867\n",
            "Training loss: 13265.332359328866\n",
            "Training loss: 13223.544266559184\n",
            "Training loss: 13158.249276332557\n",
            "Training loss: 13113.720120027661\n",
            "Training loss: 13082.880478098989\n",
            "Training loss: 13039.268180586398\n",
            "Training loss: 13001.805877000093\n",
            "Training loss: 12974.138691589236\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "loss_ffn = []\n",
        "num_epochs_ffn = 20\n",
        "\n",
        "for epoch in range(num_epochs_ffn):\n",
        "    running_loss_ffn = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(dataloader_train):\n",
        "        inputs, labels = data\n",
        "        optimizer_ffn.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = feedforward_net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_ffn.step()\n",
        "        running_loss_ffn += loss.item()\n",
        "\n",
        "    print(f\"Training loss: {running_loss_ffn}\")\n",
        "    loss_ffn.append(running_loss_ffn)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "torch.save(feedforward_net.state_dict(), 'ffn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVGD6llONliO",
        "outputId": "baa0e1e5-055c-4322-d4a4-b24b016ea572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79     24583\n",
            "           1       0.81      0.83      0.82     28247\n",
            "           2       0.72      0.48      0.58      6877\n",
            "           3       0.72      0.70      0.71     11629\n",
            "           4       0.70      0.69      0.69      9576\n",
            "           5       0.60      0.56      0.58      3133\n",
            "\n",
            "    accuracy                           0.76     84045\n",
            "   macro avg       0.72      0.68      0.70     84045\n",
            "weighted avg       0.76      0.76      0.75     84045\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    inputs = torch.from_numpy(X_test)\n",
        "    output = feedforward_net(inputs)\n",
        "    y_pred = output.argmax(1)\n",
        "    y_true = torch.tensor(y_test.to_list())\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mISxV9-wNliO",
        "outputId": "9e712336-c374-42bb-a0e8-9725981a5477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Actual  Predicted\n",
            "412679       4          4\n",
            "346836       0          0\n",
            "80692        1          1\n",
            "292510       2          1\n",
            "238292       5          5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "predicted_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(predicted_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLRqOLjuNliP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZpXIqsRNliR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_saJajj8NliS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBG1SCfWNliS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yckhhRMGNliS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ith-wVRGNliS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8uag5bGNliS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ox9iXeLNliT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKULMq23NliT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWS5nGPSNliU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8r3396CNliU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_1Pb4AMNliV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcZZT3d9NliV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
