{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7IpVqTUU29u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "from collections import defaultdict\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "label_mapping = {\n",
        "    0: 'sadness',\n",
        "    1: 'joy',\n",
        "    2: 'love',\n",
        "    3: 'anger',\n",
        "    4: 'fear',\n",
        "    5: 'surprise'\n",
        "}\n",
        "\n",
        "\n",
        "music_recommendations = {\n",
        "    'sadness': ['Melancholic Piano', 'Sad Violin Music'],\n",
        "    'joy': ['Happy Acoustic Guitar', 'Uplifting Piano'],\n",
        "    'love': ['Romantic Piano', 'Love Songs Instrumental'],\n",
        "    'anger': ['Intense Rock Instrumental', 'Heavy Metal Instrumental'],\n",
        "    'fear': ['Dark Cinematic Music', 'Tense Ambient Soundscapes'],\n",
        "    'surprise': ['Energetic Orchestral Music', 'Exciting Electronic Beats'],\n",
        "}\n",
        "\n",
        "\n",
        "data = pd.read_csv('emotions.csv')\n",
        "\n",
        "\n",
        "sample_size = 1000\n",
        "\n",
        "\n",
        "data_sample = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    contractions = {\n",
        "        \"dont\": \"do not\",\n",
        "        \"cant\": \"cannot\",\n",
        "        \"wont\": \"will not\",\n",
        "        \"im\": \"i am\",\n",
        "        \"ive\": \"i have\",\n",
        "        \"id\": \"i would\",\n",
        "        \"youre\": \"you are\",\n",
        "        \"isnt\": \"is not\",\n",
        "        \"wasnt\": \"was not\",\n",
        "        \"shouldnt\": \"should not\",\n",
        "        \"couldnt\": \"could not\",\n",
        "        \"doesnt\": \"does not\",\n",
        "        \"havent\": \"have not\",\n",
        "        \"hasnt\": \"has not\",\n",
        "        \"hadnt\": \"had not\",\n",
        "        \"arent\": \"are not\",\n",
        "        \"werent\": \"were not\",\n",
        "        \"wouldnt\": \"would not\",\n",
        "        \"mustnt\": \"must not\",\n",
        "        \"mightnt\": \"might not\",\n",
        "        \"didnt\": \"did not\",\n",
        "        \"neednt\": \"need not\",\n",
        "        \"oughtnt\": \"ought not\",\n",
        "        \"im\": \"i am\",\n",
        "        \"hes\": \"he is\",\n",
        "        \"shes\": \"she is\",\n",
        "        \"its\": \"it is\",\n",
        "        \"thats\": \"that is\",\n",
        "        \"theres\": \"there is\",\n",
        "        \"whats\": \"what is\",\n",
        "        \"wheres\": \"where is\",\n",
        "        \"whos\": \"who is\",\n",
        "        \"theyre\": \"they are\",\n",
        "        \"weve\": \"we have\",\n",
        "        \"were\": \"we are\",\n",
        "    }\n",
        "\n",
        "\n",
        "    for contraction, replacement in contractions.items():\n",
        "        text = re.sub(r'\\b' + contraction + r'\\b', replacement, text)\n",
        "\n",
        "\n",
        "    tokens = text.split()\n",
        "    tokens = handle_negations(tokens)\n",
        "\n",
        "\n",
        "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
        "\n",
        "\n",
        "    tokens = [token for token in tokens if token and token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def handle_negations(tokens):\n",
        "    negation_words = set(['no', 'not', 'never', 'none', 'cannot', 'dont', 'do not'])\n",
        "    transformed_tokens = []\n",
        "    negate = False\n",
        "    for token in tokens:\n",
        "        if token in negation_words:\n",
        "            negate = True\n",
        "        elif negate:\n",
        "            transformed_tokens.append('not_' + token)\n",
        "            negate = False\n",
        "        else:\n",
        "            transformed_tokens.append(token)\n",
        "    return transformed_tokens\n",
        "\n",
        "\n",
        "documents = data_sample['text'].tolist()\n",
        "processed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "\n",
        "vocab = set()\n",
        "for doc in processed_docs:\n",
        "    vocab.update(doc)\n",
        "vocab = list(vocab)\n",
        "vocab_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id_to_vocab = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "documents_word_ids = [[vocab_to_id[word] for word in doc if word in vocab_to_id] for doc in processed_docs]\n",
        "\n",
        "K = 10\n",
        "\n",
        "alpha = 0.1\n",
        "beta = 0.1\n",
        "\n",
        "\n",
        "D = len(documents_word_ids)\n",
        "V = len(vocab)\n",
        "\n",
        "N_dk = np.zeros((D, K)) + alpha\n",
        "N_kw = np.zeros((K, V)) + beta\n",
        "N_k = np.zeros(K) + V * beta\n",
        "\n",
        "\n",
        "topic_assignments = []\n",
        "for d, doc in enumerate(documents_word_ids):\n",
        "    current_doc_topics = []\n",
        "    for w in doc:\n",
        "        k = random.randint(0, K - 1)\n",
        "        N_dk[d, k] += 1\n",
        "        N_kw[k, w] += 1\n",
        "        N_k[k] += 1\n",
        "        current_doc_topics.append(k)\n",
        "    topic_assignments.append(current_doc_topics)\n",
        "\n",
        "\n",
        "def gibbs_sampling(iterations):\n",
        "    for it in range(iterations):\n",
        "        for d, doc in enumerate(documents_word_ids):\n",
        "            for i, w in enumerate(doc):\n",
        "                k = topic_assignments[d][i]\n",
        "\n",
        "                N_dk[d, k] -= 1\n",
        "                N_kw[k, w] -= 1\n",
        "                N_k[k] -= 1\n",
        "\n",
        "                left = N_kw[:, w] / N_k\n",
        "                right = N_dk[d, :] / np.sum(N_dk[d, :])\n",
        "                p_k = left * right\n",
        "                p_k /= np.sum(p_k)\n",
        "\n",
        "                new_k = np.random.choice(np.arange(K), p=p_k)\n",
        "\n",
        "                N_dk[d, new_k] += 1\n",
        "                N_kw[new_k, w] += 1\n",
        "                N_k[new_k] += 1\n",
        "\n",
        "                topic_assignments[d][i] = new_k\n",
        "        if (it + 1) % 10 == 0:\n",
        "            print(f\"Iteration {it + 1} completed.\")\n",
        "\n",
        "iterations = 100\n",
        "gibbs_sampling(iterations)\n",
        "\n",
        "topic_to_emotion = {}\n",
        "for k in range(K):\n",
        "    topic_docs = [d for d in range(D) if np.argmax(N_dk[d, :]) == k]\n",
        "    labels = data_sample.iloc[topic_docs]['label'].tolist()\n",
        "    if labels:\n",
        "        labels_counts = np.bincount(labels)\n",
        "        majority_label = np.argmax(labels_counts)\n",
        "        topic_to_emotion[k] = label_mapping[majority_label]\n",
        "    else:\n",
        "        topic_to_emotion[k] = 'unknown'\n",
        "\n",
        "print(\"Topic to Emotion Mapping:\")\n",
        "for k in range(K):\n",
        "    print(f\"Topic {k}: {topic_to_emotion[k]}\")\n",
        "\n",
        "def predict_emotion(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    word_ids = [vocab_to_id[word] for word in tokens if word in vocab_to_id]\n",
        "\n",
        "    N_dk_new = np.zeros(K) + alpha\n",
        "\n",
        "    topic_assignments_new = []\n",
        "    for w in word_ids:\n",
        "        k = random.randint(0, K - 1)\n",
        "        N_dk_new[k] += 1\n",
        "        topic_assignments_new.append(k)\n",
        "\n",
        "    iterations_new = 20\n",
        "    for it in range(iterations_new):\n",
        "        for i, w in enumerate(word_ids):\n",
        "            k = topic_assignments_new[i]\n",
        "\n",
        "            N_dk_new[k] -= 1\n",
        "\n",
        "            left = N_kw[:, w] / N_k\n",
        "            right = N_dk_new / np.sum(N_dk_new)\n",
        "            p_k = left * right\n",
        "            p_k /= np.sum(p_k)\n",
        "\n",
        "            new_k = np.random.choice(np.arange(K), p=p_k)\n",
        "\n",
        "            N_dk_new[new_k] += 1\n",
        "            topic_assignments_new[i] = new_k\n",
        "\n",
        "    theta_new = N_dk_new / np.sum(N_dk_new)\n",
        "\n",
        "    pred_topic = np.argmax(theta_new)\n",
        "    pred_emotion = topic_to_emotion.get(pred_topic, 'unknown')\n",
        "\n",
        "    return pred_emotion\n",
        "\n",
        "def compute_accuracy():\n",
        "    correct = 0\n",
        "    for d in range(D):\n",
        "        pred_topic = np.argmax(N_dk[d, :])\n",
        "        pred_emotion = topic_to_emotion.get(pred_topic, 'unknown')\n",
        "        true_label = data_sample.iloc[d]['label']\n",
        "        true_emotion = label_mapping[true_label]\n",
        "        if pred_emotion == true_emotion:\n",
        "            correct += 1\n",
        "    accuracy = correct / D\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "compute_accuracy()\n"
      ]
    }
  ]
}