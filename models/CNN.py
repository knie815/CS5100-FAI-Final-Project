# -*- coding: utf-8 -*-
"""LDAcnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FJ_1Ouj1UsX1LOzD85mhZFZJgfhmpo5u
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import tensorflow as tf


nltk.download('stopwords')
nltk.download('punkt')


data = pd.read_csv('processed_emotions_dataset_2.csv', index_col=0)
data = data.dropna()  
docs = data['text']  
labels = data['label']  

stop_words = set(stopwords.words('english'))
negation_words = ['not', "don't", 'no', 'never', "can't", "won't"]
additional_stopwords = {
    'really', 'very', 'totally', 'absolutely', 'so', 'quite', 'extremely',
    'incredibly', 'super', 'definitely', 'omg', 'lol', 'lmao', 'brb', 'fml',
    'idk', 'smh', 'tbh', 'like', 'you', 'know', 'um', 'literally', 'actually',
    'basically', 'kind', 'sorta', 'might', 'would', 'could', 'can', 'should',
    'not', 'no', 'donâ€™t', 'never', 'nothing', 'feel', 'felt', 'wish', 'hope',
    'maybe', 'perhaps', 'im', 'uh', 'oh', 'wow', 'ouch', 'eh', 'whoa', 'yikes'
}
stop_words.update(additional_stopwords)

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = []
    negation = False
    for word in tokens:
        if word in negation_words:
            negation = not negation  
            continue  
        if word.isalpha() and word not in stop_words:
            if negation:
                word = 'not_' + word 
            filtered_tokens.append(word)
            negation = False 
    return ' '.join(filtered_tokens)

preprocessed_docs = [preprocess_text(doc) for doc in docs]

max_words = 10000  
max_sequence_length = 100  

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(preprocessed_docs)
sequences = tokenizer.texts_to_sequences(preprocessed_docs)

X = pad_sequences(sequences, maxlen=max_sequence_length)

y = to_categorical(labels, num_classes=6) 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=4))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(6, activation='softmax')) 

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test accuracy: {accuracy * 100:.2f}%")

test_text = "I am feeling great today!"
processed_test_text = preprocess_text(test_text)
test_sequence = tokenizer.texts_to_sequences([processed_test_text])
test_padded = pad_sequences(test_sequence, maxlen=max_sequence_length)
prediction = model.predict(test_padded)
predicted_label = np.argmax(prediction, axis=1)
print(f"Predicted Label for the test text: {predicted_label[0]}")

